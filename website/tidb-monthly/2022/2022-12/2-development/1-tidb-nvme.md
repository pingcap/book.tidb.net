---
title: 百TB级TiDB集群在线更换NVME磁盘优化实践 - TiDB 社区技术月刊
sidebar_label:  百TB级TiDB集群在线更换NVME磁盘优化实践
hide_title: true
description: 本文分享了为了减少硬件成本，替换服务器资源为 NVME 方案，选用了一个 160TB 的 HTAP 集群作为试点的实践。
keywords: [TiDB, 集群, 在线更换, NVME 磁盘 ]
---

# 百TB级TiDB集群在线更换NVME磁盘优化实践

> 作者：[Jellybean](https://tidb.net/u/Jellybean/answer)

## 前言

在全球经济不景气，中国经济发展放缓的大背景下，各行各业都在通过不同方式进行降本增效。

我们就是在这样的大目标下，对公司的 TiDB 集群进行了不同的优化实践。前期我们根据业务特点，不断优化SQL、优化业务，或优化集群参数、升级 TiDB 集群版本，可以说有了不小的成效。

但是硬件成本仍然是我们的大头，所以为减少服务器数量，或降低集群的资源消耗以保障相同服务器前提下可以承载更多的业务，我们提出了将原来使用的普通 SSD 升级替换为容量更大、性能更好的 NVME 的方案。我们综合评估了业务类型和负载特征后，选用了一个 160TB 的 HTAP 集群作为试点。在方案上线运行一段时间后集群运行稳定、良好，其优化效果非常明细，在此分享我们的优化实践给大家，希望可以为有相同需求的场景提供参考经验。

## 结论

- NVME 磁盘性能表现很优秀，给 TiDB 集群的优化很明显，从集群监控指标来看：

  - 在有新 Flink 业务不断上线，集群平均 QPS 由 1 万上涨为 2 万的前提下，集群**平均 IO 由 85% 下降到 20%**，**平均 CPU 由 1000% 下降为 500%**，优化的效果非常夸张！
  - 此外，集群的 **80% 请求的延迟更平滑，毛刺和抖动更少**，且始终保留在 3ms 的较低水平线附近，集群访问更稳定
  - 对于集群 99.9% 的请求，**平均延迟由 803ms 降低为 333ms，延迟下降 59%**，集群SQL的总体执行速度有不小的提升

- TiDB 上的业务优化也很明显：

  - 某大数据分析场景单条 SQL 每日处理 2 亿行以上的数据，**处理时长由11小时缩短为9小时，效率提升18%**，有效地降低盘点耗时，避免了长时间挤兑资源而影响别的业务
  - 对一个 1 亿行的表加索引，**速度提升约 38%**，加索引耗时由 80min 下降到 50min 左右

- 相同机器数，大大提高集群可承接业务量：

  - 在空间足够的前提下，用 NVME 后所消耗的 IO、CPU 资源得到大量释放，表明相同机器数下可以接入更多的业务，保守估计该 TiDB 集群的业务接入量至少还可以**翻倍**，这大大降低未来服务器的使用成本
  - 有大量 IO 的场景，非常推荐使用 NVME 磁盘！性能上其与普通 SSD 有质的提升。

- 风险问题：

  - 使用中暂无发现问题

## 在线换盘

实施方案：

- 磁盘使用设计

  - 替换前：

    - 类型：普通 SSD，

    - 空间大小：单盘 1.8 TB

    - raid：raid 0，由集群三副本机制保证数据安全和高可用

    - 单region大小：96M

  - 替换后：

    - 类型：NVME SSD

    - 空间大小：单盘 6.4 TB

    - raid：raid 0，由集群三副本机制保证数据安全和高可用

    - 单region大小：144MB

- 在线透明、无影响换盘实施

  - 在业务低谷期，NVME 磁盘挂载在新机器上，将新机器直接扩容加入到 TiKV 集群

  - 等待集群 Region 自动调度均衡，集群稳定

  - 将普通 SSD 的 TiKV 节点陆续下线，我们为了不引起集群出现抖动，是一个一个节点操作缩容的，直到所有的节点都完成下线

  - 观察监控面板，等待集群稳定

  - 至此，耗时3天，得益于 TiDB 集群优秀的灵活扩缩能力，我们完成了百 TB 级规模的集群在线不停机换盘，期间业务无感知、无不良影响

## 效果分析

- 2022-11-22 17:39:26 执行扩容操作，新 NVME 磁盘的 TiKV 节点上线，与旧普通 SSD 的 TiKV 节点共存
- 2022-11-23 10:52:09 等待集群均衡后，执行缩容命令，下线旧普通 SSD 的 TiKV 节点
- 2022-11-24 10:05:47 清理旧普通 SSD 的 TiKV 节点数据和目录，集群 TiKV 节点完全替换为 NVME 磁盘
- 上述操作过程如下图所示：

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/1-1671081613049.jpg)

- 集群完成替换磁盘时，旧 SSD 的 TiKV 节点转入 Tombstone 状态，表明完成下线，集群此时的存储节点全部是由 NVME 磁盘提供服务

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/2-1671081876778.jpg)

- 2022-12-06 18:00:00 在集群运行两周后，检查集群使用 NVME 前后的性能情况：

1.在有新 Flink 业务不断上线，集群平均 QPS 由 1 万变为 2 万以上的前提下，集群的 80% 请求的延迟更加平滑，毛刺和抖动更少，且始终保留在 3ms 的水平线附近，说明集群数据访问更加稳定。

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/3-1671082059260.png)

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/4-1671082068915.png)

2.对于集群 99.9% 的请求，平均延迟由 803ms 降低为 333ms，延迟下降 59%，这个优化非常明显

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/5-1671082110900.png)

3.使用 NVME  给集群带来的最大优化，是 IO 下降非常明显。之前集群平均 IO 在 85% 以上且经常 IO 打满，替换后平均 IO 下降到 20% 以下，且 IO 剩余可用资源充足，优化的效果非常夸张！集群平均 CPU 由 1000% 下降为 500%，直接下降一半！内存和集群内网络流量使用保持平稳。

IO 使用对比：

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/6-1671082323003.jpg)

CPU 使用对比：

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/7-1671082349277.jpg)

内存使用对比：

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/8-1671082365300.jpg)

集群内网络带宽使用对比：

![no-alt](https://tidb-blog.oss-cn-beijing.aliyuncs.com/media/9-1671082383056.jpg)

## 最后

- NVME 磁盘性能表现很优秀，给 TiDB 集群的优化很明显！业务方也能感知到明显的优化

- 相同机器数，极大地提高集群可承接业务量，降低未来服务器的使用成本

- 有大量 IO 的场景，非常推荐使用 NVME 磁盘！性能上其与普通 SSD 有质的提升

关于其他 TiDB 集群或者系统的优化思路，推荐看晓磊老师的一篇优秀文章 [《关于 TiDB 性能优化的一些思考》](https://tidb.net/blog/148f0422)
